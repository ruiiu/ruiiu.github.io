- date: Jan 2026
  description: >-
    Four papers accepted to ICLR 2026. Congraluations to all coauthors!

- date: Jan 2026
  description: >-
    Our paper [DUPL](https://arxiv.org/pdf/2510.01444) (Dual-Uncertainty Guided Policy Learning) for multimodal LLM reasoning is out.
    
- date: Dec 2025
  description: >-
    Our paper [MSSR](https://arxiv.org/pdf/2512.18215) (Multimodal Stabilized Single-Rollout) for multimodal LLM reasoning is out.

- date: Sep 2025
  description: >-
    Three papers on (multimodal) LLMs reasoning are out. [Vision-SR1](https://arxiv.org/pdf/2508.19652) for VLM reasoning, 
    [Parallel-R1](https://zhengkid.github.io/Parallel_R1.github.io/#) for parallel thinking with RL, [CDE](https://arxiv.org/pdf/2509.09675) (Curiosity-Driven Exploration) for LLM reasoning. 

- date: Sep 2025
  description: >-
    One paper on learning under uncertainty ([AdaConG](https://arxiv.org/pdf/2502.16736)) is out.
    
- date: Sep 2025
  description: >-
    One paper on multimodal multi-agent systems ([CAML](https://arxiv.org/pdf/2502.17821)) accepted to NeurIPS 2025.

- date: Aug 2025
  description: >-
    One paper on [risk-sensitive policy gradient](https://arxiv.org/pdf/2403.08955) is out. 

- date: Jun 2025
  description: >-
    One paper on multimodal collaborative decision-making ([MMCD](https://ruiiu.github.io/mmcd/)) accepted to IROS 2025.

- date: May 2025
  description: >-
    Start my internship at Tencent AI Lab, Bellevue, WA.

- date: Jan 2025
  description: >-
    One paper on robust imitation learning for robotic manipulation ([IMRL](https://ruiiu.github.io/imrl/)) accepted to ICRA 2025.

- date: Jun 2024
  description: >-
    One paper on visual action learning for robotic manipulation ([LAVA](https://raaslab.org/projects/RoboSpoon/)) accepted to IROS 2024.

- date: Jun 2023
  description: >-
    One paper on distributionally robust optimal control ([D<sup>3</sup>ROC](https://arxiv.org/pdf/2303.02293)) accepted to IROS 2023.

- date: May 2023
  description: >-
    Start my internship at Apple, Cupertino, CA.
    
# - date: Jun 2020
#   description: >-
#     We've released the
#     [code](https://github.com/NVlabs/latentfusion) and
#     [data](https://latentfusion.github.io) for [_LatentFusion_](https://latentfusion.github.io).
# - date: Feb 2020
#   description: >-
#     [_LatentFusion_](https://latentfusion.github.io), our paper on zero-shot pose estimation,
#     has been accepted to CVPR 2020.
